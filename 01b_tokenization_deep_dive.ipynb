{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea26039",
   "metadata": {},
   "source": [
    "# Tokenization Deep Dive\n",
    "Explore how tokenization works under the hood using Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce780d",
   "metadata": {},
   "source": [
    "## Install Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c80319",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c10c44",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ae310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Playing with transformers is amazing!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d49a2",
   "metadata": {},
   "source": [
    "## Rare Word Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e82fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_word = \"transformerscape\"\n",
    "rare_tokens = tokenizer.tokenize(rare_word)\n",
    "print(\"Rare word tokens:\", rare_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd717d",
   "metadata": {},
   "source": [
    "## Convert Tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0feb83",
   "metadata": {},
   "source": [
    "## Add Special Tokens ([CLS], [SEP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c78b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_with_special = tokenizer.build_inputs_with_special_tokens(token_ids)\n",
    "print(\"Token IDs with special tokens:\", tokens_with_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a9486",
   "metadata": {},
   "source": [
    "## Full Encoding with Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6506675",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"Input IDs:\", inputs['input_ids'])\n",
    "print(\"Attention Mask:\", inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ae731",
   "metadata": {},
   "source": [
    "## Compare Tokenizers: BERT vs RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "sentence = \"Let's explore Hugging Face!\"\n",
    "\n",
    "print(\"BERT:\", bert_tokenizer.tokenize(sentence))\n",
    "print(\"RoBERTa:\", roberta_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd586559",
   "metadata": {},
   "source": [
    "## Handle Emojis, Accents, Mixed Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_text = \"I love AI ü§ñ!\"\n",
    "accent_text = \"Caf√© au lait\"\n",
    "multi_lang = \"ŸÖÿ±ÿ≠ÿ®ÿß Hello „Åì„Çì„Å´„Å°„ÅØ\"\n",
    "\n",
    "print(\"Emoji:\", tokenizer.tokenize(emoji_text))\n",
    "print(\"Accents:\", tokenizer.tokenize(accent_text))\n",
    "print(\"Mixed:\", tokenizer.tokenize(multi_lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40743823",
   "metadata": {},
   "source": [
    "## Padding & Truncation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cddfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is short.\", \"This is a much longer sentence that may need truncation.\"]\n",
    "encoded = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Padded input IDs:\", encoded['input_ids'])\n",
    "print(\"Attention mask:\", encoded['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f665d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Tokenizers break text into subwords.\n",
    "- They map tokens to IDs used by models.\n",
    "- Special tokens, attention masks, and padding help with model inputs.\n",
    "- Tokenizers differ (e.g., BERT vs RoBERTa).\n",
    "- Transformers process sequences, not raw words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
